{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![](../../../../Documents/Screenshots/screenshot.286.jpg)\n",
    "![](../../../../Documents/Screenshots/screenshot.284.jpg)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.ops.init_ops_v2 import glorot_uniform\n",
    "from keras.layers import ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from keras import Sequential\n",
    "import tensorflow as tf\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shape = (96, 96, 1)\n",
    "\n",
    "\n",
    "def convolutional_block(filters):\n",
    "    f1, f2, f3 = filters\n",
    "\n",
    "    return [\n",
    "        Conv2D(f1, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f2, (3, 3), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f3, (3, 3), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "\n",
    "        Conv2D(f3, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(axis=3),\n",
    "\n",
    "        Activation('relu')\n",
    "    ]\n",
    "\n",
    "\n",
    "def identity_block(filters):\n",
    "    f1, f2, f3 = filters\n",
    "\n",
    "    return [\n",
    "        Conv2D(f1, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f2, (3, 3), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f3, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu')\n",
    "    ]\n",
    "\n",
    "\n",
    "def res_block(model, filters):\n",
    "    layers = []\n",
    "\n",
    "    layers.extend(convolutional_block(filters))\n",
    "    layers.extend(identity_block(filters))\n",
    "    layers.extend(identity_block(filters))\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((3, 3), input_shape=shape))\n",
    "    model.add(Conv2D(64, (3, 3), name='conv1'))\n",
    "    model.add(BatchNormalization(axis=3, name='bn_conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((3, 3)))\n",
    "\n",
    "    res_block(model, [64, 64, 256])\n",
    "    res_block(model, [128, 128, 512])\n",
    "\n",
    "    model.add(AveragePooling2D((2, 2), name='avg_pool'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPadding  (None, 102, 102, 1)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 100, 100, 64)      640       \n",
      "                                                                 \n",
      " bn_conv1 (BatchNormalizatio  (None, 100, 100, 64)     256       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100, 100, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 33, 33, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 33, 33, 64)        4160      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 16, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 256)       147712    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 256)       65792     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 256)         16640     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 256)         16640     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 128)         32896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 4, 4, 512)         590336    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 4, 4, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 512)         262656    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 2, 2, 128)         65664     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 2, 2, 512)         66048     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 2, 2, 128)         65664     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 2, 2, 512)         66048     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " avg_pool (AveragePooling2D)  (None, 1, 1, 512)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                61470     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,557,854\n",
      "Trainable params: 12,549,278\n",
      "Non-trainable params: 8,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "       left_eye_center_x  left_eye_center_y  right_eye_center_x  \\\n0              67.671771          34.955794           30.898286   \n1              31.254295          35.921025           61.864642   \n2              64.370699          60.584782           58.726033   \n3              67.430201          38.067488           29.395599   \n4              65.304107          36.543259           28.844745   \n...                  ...                ...                 ...   \n42795          61.780769          30.876849           23.146882   \n42796          61.713259          32.374178           28.300633   \n42797          70.205428          36.172943           29.954357   \n42798          40.323722          28.643860           32.585601   \n42799          67.690862          37.997409           30.143760   \n\n       right_eye_center_y  left_eye_inner_corner_x  left_eye_inner_corner_y  \\\n0               34.036457                61.849371                35.262309   \n1               39.444518                36.979972                37.484575   \n2               28.572293                62.453406                53.210688   \n3               39.855280                59.811322                39.393218   \n4               37.428173                57.284578                39.391575   \n...                   ...                      ...                      ...   \n42795           36.066028                54.332347                33.765992   \n42796           38.780817                55.727903                34.975987   \n42797           29.227776                62.532826                35.582372   \n42798           62.538858                39.350782                35.928832   \n42799           35.590226                59.451842                38.593001   \n\n       left_eye_outer_corner_x  left_eye_outer_corner_y  \\\n0                    73.188343                35.875200   \n1                    26.057143                35.260370   \n2                    65.883418                67.413173   \n3                    75.069343                39.042768   \n4                    74.014975                37.677055   \n...                        ...                      ...   \n42795                71.931763                30.067577   \n42796                68.139900                32.271748   \n42797                79.000536                38.181007   \n42798                41.711117                20.904723   \n42799                75.843605                39.305064   \n\n       right_eye_inner_corner_x  right_eye_inner_corner_y  ...  nose_tip_y  \\\n0                     37.333714                 35.262309  ...   53.649600   \n1                     57.019839                 39.290365  ...   52.060657   \n2                     58.584578                 34.766968  ...   46.151078   \n3                     38.021315                 40.347239  ...   59.035017   \n4                     38.717063                 39.723418  ...   60.757739   \n...                         ...                       ...  ...         ...   \n42795                 34.192934                 36.880099  ...   61.106977   \n42796                 35.020965                 38.885013  ...   52.852166   \n42797                 38.497192                 31.801490  ...   56.575488   \n42798                 34.465993                 56.134493  ...   49.518453   \n42799                 37.092332                 36.706961  ...   59.605044   \n\n       mouth_left_corner_x  mouth_left_corner_y  mouth_right_corner_x  \\\n0                62.155886            76.020343             35.494629   \n1                35.134884            75.526531             59.572169   \n2                28.029345            62.673796             24.706874   \n3                70.786215            72.635128             31.340844   \n4                65.176153            72.101105             29.609314   \n...                    ...                  ...                   ...   \n42795            66.946179            78.849739             32.860750   \n42796            66.591061            67.345551             37.472421   \n42797            58.511053            77.416298             26.498234   \n42798            78.541504            38.006918             71.528758   \n42799            62.443102            77.520980             30.970776   \n\n       mouth_right_corner_y  mouth_center_top_lip_x  mouth_center_top_lip_y  \\\n0                 74.794286               48.365486               71.116800   \n1                 76.397502               41.777883               75.711058   \n2                 38.581047               29.492332               48.225583   \n3                 76.293426               51.013536               70.575612   \n4                 73.965298               48.338648               75.323869   \n...                     ...                     ...                     ...   \n42795             83.545325               52.802848               82.753299   \n42796             73.252879               51.340933               65.651955   \n42797             71.313974               42.799900               70.529005   \n42798             67.465329               75.978338               53.020691   \n42799             76.476082               46.119706               77.742916   \n\n       mouth_center_bottom_lip_x  mouth_center_bottom_lip_y  \\\n0                      48.365486                  80.923200   \n1                      43.131087                  78.807024   \n2                      19.530965                  51.216123   \n3                      52.151253                  84.066436   \n4                      48.398417                  81.397541   \n...                          ...                        ...   \n42795                  53.198861                  83.432178   \n42796                  53.354387                  75.608568   \n42797                  41.357206                  79.575985   \n42798                  76.705854                  53.548766   \n42799                  45.871020                  78.189714   \n\n                                                   Image  \n0      0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n1      0.4014 0.3982 0.3997 0.3982 0.3901 0.3801 0.38...  \n2      0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n3      0.002028 0.0007954 0.0005965 0.0005965 0.00067...  \n4      0.001878 0.00202 0.002743 0.002092 0.001203 0....  \n...                                                  ...  \n42795  0.0002618 0.0001496 0.0001122 0.0001869 0.0002...  \n42796  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n42797  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0004797 0.00...  \n42798  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.001392 0.001...  \n42799  0.2744 0.2666 0.2705 0.2666 0.2627 0.2588 0.25...  \n\n[42800 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>left_eye_center_x</th>\n      <th>left_eye_center_y</th>\n      <th>right_eye_center_x</th>\n      <th>right_eye_center_y</th>\n      <th>left_eye_inner_corner_x</th>\n      <th>left_eye_inner_corner_y</th>\n      <th>left_eye_outer_corner_x</th>\n      <th>left_eye_outer_corner_y</th>\n      <th>right_eye_inner_corner_x</th>\n      <th>right_eye_inner_corner_y</th>\n      <th>...</th>\n      <th>nose_tip_y</th>\n      <th>mouth_left_corner_x</th>\n      <th>mouth_left_corner_y</th>\n      <th>mouth_right_corner_x</th>\n      <th>mouth_right_corner_y</th>\n      <th>mouth_center_top_lip_x</th>\n      <th>mouth_center_top_lip_y</th>\n      <th>mouth_center_bottom_lip_x</th>\n      <th>mouth_center_bottom_lip_y</th>\n      <th>Image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>67.671771</td>\n      <td>34.955794</td>\n      <td>30.898286</td>\n      <td>34.036457</td>\n      <td>61.849371</td>\n      <td>35.262309</td>\n      <td>73.188343</td>\n      <td>35.875200</td>\n      <td>37.333714</td>\n      <td>35.262309</td>\n      <td>...</td>\n      <td>53.649600</td>\n      <td>62.155886</td>\n      <td>76.020343</td>\n      <td>35.494629</td>\n      <td>74.794286</td>\n      <td>48.365486</td>\n      <td>71.116800</td>\n      <td>48.365486</td>\n      <td>80.923200</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31.254295</td>\n      <td>35.921025</td>\n      <td>61.864642</td>\n      <td>39.444518</td>\n      <td>36.979972</td>\n      <td>37.484575</td>\n      <td>26.057143</td>\n      <td>35.260370</td>\n      <td>57.019839</td>\n      <td>39.290365</td>\n      <td>...</td>\n      <td>52.060657</td>\n      <td>35.134884</td>\n      <td>75.526531</td>\n      <td>59.572169</td>\n      <td>76.397502</td>\n      <td>41.777883</td>\n      <td>75.711058</td>\n      <td>43.131087</td>\n      <td>78.807024</td>\n      <td>0.4014 0.3982 0.3997 0.3982 0.3901 0.3801 0.38...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>64.370699</td>\n      <td>60.584782</td>\n      <td>58.726033</td>\n      <td>28.572293</td>\n      <td>62.453406</td>\n      <td>53.210688</td>\n      <td>65.883418</td>\n      <td>67.413173</td>\n      <td>58.584578</td>\n      <td>34.766968</td>\n      <td>...</td>\n      <td>46.151078</td>\n      <td>28.029345</td>\n      <td>62.673796</td>\n      <td>24.706874</td>\n      <td>38.581047</td>\n      <td>29.492332</td>\n      <td>48.225583</td>\n      <td>19.530965</td>\n      <td>51.216123</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>67.430201</td>\n      <td>38.067488</td>\n      <td>29.395599</td>\n      <td>39.855280</td>\n      <td>59.811322</td>\n      <td>39.393218</td>\n      <td>75.069343</td>\n      <td>39.042768</td>\n      <td>38.021315</td>\n      <td>40.347239</td>\n      <td>...</td>\n      <td>59.035017</td>\n      <td>70.786215</td>\n      <td>72.635128</td>\n      <td>31.340844</td>\n      <td>76.293426</td>\n      <td>51.013536</td>\n      <td>70.575612</td>\n      <td>52.151253</td>\n      <td>84.066436</td>\n      <td>0.002028 0.0007954 0.0005965 0.0005965 0.00067...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>65.304107</td>\n      <td>36.543259</td>\n      <td>28.844745</td>\n      <td>37.428173</td>\n      <td>57.284578</td>\n      <td>39.391575</td>\n      <td>74.014975</td>\n      <td>37.677055</td>\n      <td>38.717063</td>\n      <td>39.723418</td>\n      <td>...</td>\n      <td>60.757739</td>\n      <td>65.176153</td>\n      <td>72.101105</td>\n      <td>29.609314</td>\n      <td>73.965298</td>\n      <td>48.338648</td>\n      <td>75.323869</td>\n      <td>48.398417</td>\n      <td>81.397541</td>\n      <td>0.001878 0.00202 0.002743 0.002092 0.001203 0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>42795</th>\n      <td>61.780769</td>\n      <td>30.876849</td>\n      <td>23.146882</td>\n      <td>36.066028</td>\n      <td>54.332347</td>\n      <td>33.765992</td>\n      <td>71.931763</td>\n      <td>30.067577</td>\n      <td>34.192934</td>\n      <td>36.880099</td>\n      <td>...</td>\n      <td>61.106977</td>\n      <td>66.946179</td>\n      <td>78.849739</td>\n      <td>32.860750</td>\n      <td>83.545325</td>\n      <td>52.802848</td>\n      <td>82.753299</td>\n      <td>53.198861</td>\n      <td>83.432178</td>\n      <td>0.0002618 0.0001496 0.0001122 0.0001869 0.0002...</td>\n    </tr>\n    <tr>\n      <th>42796</th>\n      <td>61.713259</td>\n      <td>32.374178</td>\n      <td>28.300633</td>\n      <td>38.780817</td>\n      <td>55.727903</td>\n      <td>34.975987</td>\n      <td>68.139900</td>\n      <td>32.271748</td>\n      <td>35.020965</td>\n      <td>38.885013</td>\n      <td>...</td>\n      <td>52.852166</td>\n      <td>66.591061</td>\n      <td>67.345551</td>\n      <td>37.472421</td>\n      <td>73.252879</td>\n      <td>51.340933</td>\n      <td>65.651955</td>\n      <td>53.354387</td>\n      <td>75.608568</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n    </tr>\n    <tr>\n      <th>42797</th>\n      <td>70.205428</td>\n      <td>36.172943</td>\n      <td>29.954357</td>\n      <td>29.227776</td>\n      <td>62.532826</td>\n      <td>35.582372</td>\n      <td>79.000536</td>\n      <td>38.181007</td>\n      <td>38.497192</td>\n      <td>31.801490</td>\n      <td>...</td>\n      <td>56.575488</td>\n      <td>58.511053</td>\n      <td>77.416298</td>\n      <td>26.498234</td>\n      <td>71.313974</td>\n      <td>42.799900</td>\n      <td>70.529005</td>\n      <td>41.357206</td>\n      <td>79.575985</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0004797 0.00...</td>\n    </tr>\n    <tr>\n      <th>42798</th>\n      <td>40.323722</td>\n      <td>28.643860</td>\n      <td>32.585601</td>\n      <td>62.538858</td>\n      <td>39.350782</td>\n      <td>35.928832</td>\n      <td>41.711117</td>\n      <td>20.904723</td>\n      <td>34.465993</td>\n      <td>56.134493</td>\n      <td>...</td>\n      <td>49.518453</td>\n      <td>78.541504</td>\n      <td>38.006918</td>\n      <td>71.528758</td>\n      <td>67.465329</td>\n      <td>75.978338</td>\n      <td>53.020691</td>\n      <td>76.705854</td>\n      <td>53.548766</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.001392 0.001...</td>\n    </tr>\n    <tr>\n      <th>42799</th>\n      <td>67.690862</td>\n      <td>37.997409</td>\n      <td>30.143760</td>\n      <td>35.590226</td>\n      <td>59.451842</td>\n      <td>38.593001</td>\n      <td>75.843605</td>\n      <td>39.305064</td>\n      <td>37.092332</td>\n      <td>36.706961</td>\n      <td>...</td>\n      <td>59.605044</td>\n      <td>62.443102</td>\n      <td>77.520980</td>\n      <td>30.970776</td>\n      <td>76.476082</td>\n      <td>46.119706</td>\n      <td>77.742916</td>\n      <td>45.871020</td>\n      <td>78.189714</td>\n      <td>0.2744 0.2666 0.2705 0.2666 0.2627 0.2588 0.25...</td>\n    </tr>\n  </tbody>\n</table>\n<p>42800 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_feather('datasets/augmented_data.feather').copy()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_in_batches = np.array_split(data, 50)\n",
    "\n",
    "train_xs = []\n",
    "train_ys = []\n",
    "test_xs = []\n",
    "test_ys = []\n",
    "\n",
    "index = 0\n",
    "for batch in data_in_batches:\n",
    "    images = batch['Image'].str.split(' ', expand=True).astype(np.float32).to_numpy().reshape(-1, 96, 96, 1)\n",
    "    points = batch.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(images, points, train_size=0.8)\n",
    "    train_xs.extend(train_x.tolist())\n",
    "    train_ys.extend(train_y.tolist())\n",
    "    test_xs.extend(test_x.tolist())\n",
    "    test_ys.extend(test_y.tolist())\n",
    "\n",
    "    index += 1\n",
    "    print(index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train_on_batch(train_xs, train_ys, epochs=200, validation_data=(test_xs, test_ys))\n",
    "model.save('models/facialPointsPredictor.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
