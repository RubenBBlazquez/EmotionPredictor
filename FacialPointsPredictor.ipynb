{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![](../../../../Documents/Screenshots/screenshot.286.jpg)\n",
    "![](../../../../Documents/Screenshots/screenshot.284.jpg)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.ops.init_ops_v2 import glorot_uniform\n",
    "from keras.layers import ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from keras import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "shape = (96, 96, 1)\n",
    "\n",
    "\n",
    "def convolutional_block(filters):\n",
    "    f1, f2, f3 = filters\n",
    "\n",
    "    return [\n",
    "        Conv2D(f1, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f2, (3, 3), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f3, (3, 3), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "\n",
    "        Conv2D(f3, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        BatchNormalization(axis=3),\n",
    "\n",
    "        Activation('relu')\n",
    "    ]\n",
    "\n",
    "\n",
    "def identity_block(filters):\n",
    "    f1, f2, f3 = filters\n",
    "\n",
    "    return [\n",
    "        Conv2D(f1, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f2, (3, 3), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Conv2D(f3, (1, 1), strides=(1, 1), kernel_initializer=glorot_uniform(seed=0)),\n",
    "        BatchNormalization(axis=3),\n",
    "        Activation('relu')\n",
    "    ]\n",
    "\n",
    "\n",
    "def res_block(model, filters):\n",
    "    layers = []\n",
    "\n",
    "    layers.extend(convolutional_block(filters))\n",
    "    layers.extend(identity_block(filters))\n",
    "    layers.extend(identity_block(filters))\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((3, 3), input_shape=shape))\n",
    "    model.add(Conv2D(64, (3, 3), name='conv1'))\n",
    "    model.add(BatchNormalization(axis=3, name='bn_conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((3, 3)))\n",
    "\n",
    "    res_block(model, [64, 64, 256])\n",
    "    res_block(model, [128, 128, 512])\n",
    "\n",
    "    model.add(AveragePooling2D((2, 2), name='avg_pool'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPadding  (None, 102, 102, 1)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 100, 100, 64)      640       \n",
      "                                                                 \n",
      " bn_conv1 (BatchNormalizatio  (None, 100, 100, 64)     256       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100, 100, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 33, 33, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 33, 33, 64)        4160      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 16, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 256)       147712    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 256)       65792     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 256)         16640     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 64)          16448     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 256)         16640     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 128)         32896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 4, 4, 512)         590336    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 4, 4, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 512)         262656    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 2, 2, 128)         65664     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 2, 2, 512)         66048     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 2, 2, 128)         65664     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 2, 2, 512)         66048     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " avg_pool (AveragePooling2D)  (None, 1, 1, 512)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              2101248   \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              8390656   \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                61470     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,557,854\n",
      "Trainable params: 12,549,278\n",
      "Non-trainable params: 8,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       left_eye_center_x  left_eye_center_y  right_eye_center_x  \\\n0              66.726353          39.959788           30.132000   \n1              26.122504          41.444354           46.891566   \n2              67.644923          35.806769           30.516923   \n3              59.060656          28.068398           61.703977   \n4              34.659778          35.784862           68.369724   \n...                  ...                ...                 ...   \n30019          68.125277          35.978860           31.155064   \n30020          64.794504          39.382351           30.064489   \n30021          66.793486          57.273753           43.908208   \n30022          63.764706          37.941929           28.012235   \n30023          66.323146          34.143196           27.668094   \n\n       right_eye_center_y  left_eye_inner_corner_x  left_eye_inner_corner_y  \\\n0               39.328871                60.416471                39.328871   \n1               65.433057                30.441075                45.408925   \n2               33.056615                61.594462                36.081846   \n3               67.311469                58.653998                36.811678   \n4               36.415190                39.904107                36.843813   \n...                   ...                      ...                      ...   \n30019           36.891677                62.648170                35.978860   \n30020           38.714382                57.781740                41.052092   \n30021           25.890490                61.589613                52.442580   \n30022           37.941929                59.558588                38.783153   \n30023           34.913728                58.104131                35.581523   \n\n       left_eye_outer_corner_x  left_eye_outer_corner_y  \\\n0                    73.385647                41.013529   \n1                    22.368487                37.053585   \n2                    72.595077                37.182154   \n3                    57.637308                20.341536   \n4                    22.759184                37.045518   \n...                        ...                      ...   \n30019                74.515404                36.891677   \n30020                73.143206                40.384122   \n30021                70.045194                62.975725   \n30022                71.811765                37.510588   \n30023                75.723644                35.401732   \n\n       right_eye_inner_corner_x  right_eye_inner_corner_y  ...  nose_tip_y  \\\n0                     37.703294                 38.066965  ...   56.994847   \n1                     42.594146                 61.025747  ...   34.464890   \n2                     35.742769                 34.706462  ...   57.154462   \n3                     60.077300                 58.974938  ...   50.231659   \n4                     59.444278                 37.726272  ...   59.821648   \n...                         ...                       ...  ...         ...   \n30019                 37.088681                 38.260902  ...   55.971677   \n30020                 37.411053                 40.384122  ...   63.092519   \n30021                 46.935294                 31.780944  ...   55.306622   \n30022                 34.321882                 38.362541  ...   52.243012   \n30023                 35.912792                 35.889736  ...   58.765953   \n\n       mouth_left_corner_x  mouth_left_corner_y  mouth_right_corner_x  \\\n0                62.625176            76.553647             32.971059   \n1                57.968523            13.661815             73.980893   \n2                58.294154            74.859692             33.817231   \n3                16.767697            34.168265             18.190863   \n4                39.275433            76.962439             48.939912   \n...                    ...                  ...                   ...   \n30019            65.842723            72.948766             37.088681   \n30020            64.126901            81.459298             31.066260   \n30021            26.049628            79.964947             11.627461   \n30022            63.764706            72.012235             34.321882   \n30023            61.441115            78.957491             33.142857   \n\n       mouth_right_corner_y  mouth_center_top_lip_x  mouth_center_top_lip_y  \\\n0                 76.869176               48.113647               70.244471   \n1                 32.461575               61.575801               26.004255   \n2                 72.659692               45.643077               68.534154   \n3                 66.904720               21.443943               51.858199   \n4                 76.649961               46.306174               79.216740   \n...                     ...                     ...                     ...   \n30019             75.230298               50.325447               70.209702   \n30020             82.127267               47.763298               75.114504   \n30021             61.407733               23.223736               67.006301   \n30022             73.694118               50.725647               69.488000   \n30023             79.581882               45.831359               79.269686   \n\n       mouth_center_bottom_lip_x  mouth_center_bottom_lip_y  \\\n0                      47.798118                  84.440471   \n1                      65.321649                  23.011287   \n2                      44.818462                  81.735385   \n3                       8.024417                  52.671697   \n4                      46.417773                  80.020253   \n...                          ...                        ...   \n30019                  50.780936                  82.989957   \n30020                  48.097466                  89.140031   \n30021                  11.979579                  76.441269   \n30022                  51.146353                  81.265882   \n30023                  46.834843                  81.878746   \n\n                                                   Image  \n0      0.00543 0.00541 0.0055 0.00543 0.00513 0.00474...  \n1      0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n2      0.00281 0.002735 0.003036 0.004025 0.003492 0....  \n3      0.4038 0.3647 0.306 0.298 0.3137 0.3293 0.3843...  \n4      0.0745 0.0706 0.06665 0.06274 0.0745 0.08234 0...  \n...                                                  ...  \n30019  0.05286 0.0538 0.0567 0.0567 0.048 0.03674 0.0...  \n30020  0.149 0.1333 0.1255 0.1216 0.1137 0.102 0.098 ...  \n30021  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n30022  0.004425 0.004055 0.003712 0.003317 0.003078 0...  \n30023  0.5254 0.5176 0.4666 0.443 0.4158 0.3372 0.294...  \n\n[30024 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>left_eye_center_x</th>\n      <th>left_eye_center_y</th>\n      <th>right_eye_center_x</th>\n      <th>right_eye_center_y</th>\n      <th>left_eye_inner_corner_x</th>\n      <th>left_eye_inner_corner_y</th>\n      <th>left_eye_outer_corner_x</th>\n      <th>left_eye_outer_corner_y</th>\n      <th>right_eye_inner_corner_x</th>\n      <th>right_eye_inner_corner_y</th>\n      <th>...</th>\n      <th>nose_tip_y</th>\n      <th>mouth_left_corner_x</th>\n      <th>mouth_left_corner_y</th>\n      <th>mouth_right_corner_x</th>\n      <th>mouth_right_corner_y</th>\n      <th>mouth_center_top_lip_x</th>\n      <th>mouth_center_top_lip_y</th>\n      <th>mouth_center_bottom_lip_x</th>\n      <th>mouth_center_bottom_lip_y</th>\n      <th>Image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>66.726353</td>\n      <td>39.959788</td>\n      <td>30.132000</td>\n      <td>39.328871</td>\n      <td>60.416471</td>\n      <td>39.328871</td>\n      <td>73.385647</td>\n      <td>41.013529</td>\n      <td>37.703294</td>\n      <td>38.066965</td>\n      <td>...</td>\n      <td>56.994847</td>\n      <td>62.625176</td>\n      <td>76.553647</td>\n      <td>32.971059</td>\n      <td>76.869176</td>\n      <td>48.113647</td>\n      <td>70.244471</td>\n      <td>47.798118</td>\n      <td>84.440471</td>\n      <td>0.00543 0.00541 0.0055 0.00543 0.00513 0.00474...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26.122504</td>\n      <td>41.444354</td>\n      <td>46.891566</td>\n      <td>65.433057</td>\n      <td>30.441075</td>\n      <td>45.408925</td>\n      <td>22.368487</td>\n      <td>37.053585</td>\n      <td>42.594146</td>\n      <td>61.025747</td>\n      <td>...</td>\n      <td>34.464890</td>\n      <td>57.968523</td>\n      <td>13.661815</td>\n      <td>73.980893</td>\n      <td>32.461575</td>\n      <td>61.575801</td>\n      <td>26.004255</td>\n      <td>65.321649</td>\n      <td>23.011287</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>67.644923</td>\n      <td>35.806769</td>\n      <td>30.516923</td>\n      <td>33.056615</td>\n      <td>61.594462</td>\n      <td>36.081846</td>\n      <td>72.595077</td>\n      <td>37.182154</td>\n      <td>35.742769</td>\n      <td>34.706462</td>\n      <td>...</td>\n      <td>57.154462</td>\n      <td>58.294154</td>\n      <td>74.859692</td>\n      <td>33.817231</td>\n      <td>72.659692</td>\n      <td>45.643077</td>\n      <td>68.534154</td>\n      <td>44.818462</td>\n      <td>81.735385</td>\n      <td>0.00281 0.002735 0.003036 0.004025 0.003492 0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59.060656</td>\n      <td>28.068398</td>\n      <td>61.703977</td>\n      <td>67.311469</td>\n      <td>58.653998</td>\n      <td>36.811678</td>\n      <td>57.637308</td>\n      <td>20.341536</td>\n      <td>60.077300</td>\n      <td>58.974938</td>\n      <td>...</td>\n      <td>50.231659</td>\n      <td>16.767697</td>\n      <td>34.168265</td>\n      <td>18.190863</td>\n      <td>66.904720</td>\n      <td>21.443943</td>\n      <td>51.858199</td>\n      <td>8.024417</td>\n      <td>52.671697</td>\n      <td>0.4038 0.3647 0.306 0.298 0.3137 0.3293 0.3843...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>34.659778</td>\n      <td>35.784862</td>\n      <td>68.369724</td>\n      <td>36.415190</td>\n      <td>39.904107</td>\n      <td>36.843813</td>\n      <td>22.759184</td>\n      <td>37.045518</td>\n      <td>59.444278</td>\n      <td>37.726272</td>\n      <td>...</td>\n      <td>59.821648</td>\n      <td>39.275433</td>\n      <td>76.962439</td>\n      <td>48.939912</td>\n      <td>76.649961</td>\n      <td>46.306174</td>\n      <td>79.216740</td>\n      <td>46.417773</td>\n      <td>80.020253</td>\n      <td>0.0745 0.0706 0.06665 0.06274 0.0745 0.08234 0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>30019</th>\n      <td>68.125277</td>\n      <td>35.978860</td>\n      <td>31.155064</td>\n      <td>36.891677</td>\n      <td>62.648170</td>\n      <td>35.978860</td>\n      <td>74.515404</td>\n      <td>36.891677</td>\n      <td>37.088681</td>\n      <td>38.260902</td>\n      <td>...</td>\n      <td>55.971677</td>\n      <td>65.842723</td>\n      <td>72.948766</td>\n      <td>37.088681</td>\n      <td>75.230298</td>\n      <td>50.325447</td>\n      <td>70.209702</td>\n      <td>50.780936</td>\n      <td>82.989957</td>\n      <td>0.05286 0.0538 0.0567 0.0567 0.048 0.03674 0.0...</td>\n    </tr>\n    <tr>\n      <th>30020</th>\n      <td>64.794504</td>\n      <td>39.382351</td>\n      <td>30.064489</td>\n      <td>38.714382</td>\n      <td>57.781740</td>\n      <td>41.052092</td>\n      <td>73.143206</td>\n      <td>40.384122</td>\n      <td>37.411053</td>\n      <td>40.384122</td>\n      <td>...</td>\n      <td>63.092519</td>\n      <td>64.126901</td>\n      <td>81.459298</td>\n      <td>31.066260</td>\n      <td>82.127267</td>\n      <td>47.763298</td>\n      <td>75.114504</td>\n      <td>48.097466</td>\n      <td>89.140031</td>\n      <td>0.149 0.1333 0.1255 0.1216 0.1137 0.102 0.098 ...</td>\n    </tr>\n    <tr>\n      <th>30021</th>\n      <td>66.793486</td>\n      <td>57.273753</td>\n      <td>43.908208</td>\n      <td>25.890490</td>\n      <td>61.589613</td>\n      <td>52.442580</td>\n      <td>70.045194</td>\n      <td>62.975725</td>\n      <td>46.935294</td>\n      <td>31.780944</td>\n      <td>...</td>\n      <td>55.306622</td>\n      <td>26.049628</td>\n      <td>79.964947</td>\n      <td>11.627461</td>\n      <td>61.407733</td>\n      <td>23.223736</td>\n      <td>67.006301</td>\n      <td>11.979579</td>\n      <td>76.441269</td>\n      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n    </tr>\n    <tr>\n      <th>30022</th>\n      <td>63.764706</td>\n      <td>37.941929</td>\n      <td>28.012235</td>\n      <td>37.941929</td>\n      <td>59.558588</td>\n      <td>38.783153</td>\n      <td>71.811765</td>\n      <td>37.510588</td>\n      <td>34.321882</td>\n      <td>38.362541</td>\n      <td>...</td>\n      <td>52.243012</td>\n      <td>63.764706</td>\n      <td>72.012235</td>\n      <td>34.321882</td>\n      <td>73.694118</td>\n      <td>50.725647</td>\n      <td>69.488000</td>\n      <td>51.146353</td>\n      <td>81.265882</td>\n      <td>0.004425 0.004055 0.003712 0.003317 0.003078 0...</td>\n    </tr>\n    <tr>\n      <th>30023</th>\n      <td>66.323146</td>\n      <td>34.143196</td>\n      <td>27.668094</td>\n      <td>34.913728</td>\n      <td>58.104131</td>\n      <td>35.581523</td>\n      <td>75.723644</td>\n      <td>35.401732</td>\n      <td>35.912792</td>\n      <td>35.889736</td>\n      <td>...</td>\n      <td>58.765953</td>\n      <td>61.441115</td>\n      <td>78.957491</td>\n      <td>33.142857</td>\n      <td>79.581882</td>\n      <td>45.831359</td>\n      <td>79.269686</td>\n      <td>46.834843</td>\n      <td>81.878746</td>\n      <td>0.5254 0.5176 0.4666 0.443 0.4158 0.3372 0.294...</td>\n    </tr>\n  </tbody>\n</table>\n<p>30024 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_feather('datasets/augmented_data.feather').copy()\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m----> 3\u001B[0m data_in_batches \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39marray_split(data, \u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m      5\u001B[0m train_xs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m train_ys \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_in_batches = np.array_split(data, 50)\n",
    "\n",
    "train_xs = []\n",
    "train_ys = []\n",
    "test_xs = []\n",
    "test_ys = []\n",
    "\n",
    "for batch in data_in_batches:\n",
    "    images = batch['Image'].str.split(' ', expand=True).astype(np.float32).to_numpy().reshape(-1, 96, 96, 1)\n",
    "    points = batch.iloc[:, :-1].to_numpy()\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(images, points, train_size=0.8)\n",
    "\n",
    "    train_xs = np.concatenate((train_xs, train_x))\n",
    "    train_ys = np.concatenate((train_ys, train_y))\n",
    "    test_xs = np.concatenate((test_xs, test_x))\n",
    "    test_ys = np.concatenate((test_ys, test_y))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(train_xs, train_ys, epochs=200, validation_data=(test_xs, test_ys))\n",
    "model.save('models/facialPointsPredictor.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
